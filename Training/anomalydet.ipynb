{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "USING ANOMALY DETECTION\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of anomalies detected: 12617\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import pandas as pd\n",
    "from sklearn.ensemble import IsolationForest\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Loading the dataset\n",
    "dataset = pd.read_csv('D:/Education/PYTHON/ML-v2/Datasets/malicious_phish.csv')\n",
    "\n",
    "# Extracting domain-based features\n",
    "dataset['domain_length'] = dataset['url'].apply(lambda x: len(re.findall('/.', x)))\n",
    "dataset['subdomains'] = dataset['url'].apply(lambda x: x.count('.') - 1)\n",
    "dataset['has_ip'] = dataset['url'].apply(lambda x: 1 if re.match(r'\\b\\d{1,3}\\.\\d{1,3}\\.\\d{1,3}\\.\\d{1,3}\\b', x) else 0)\n",
    "\n",
    "# Extracting path-based features\n",
    "dataset['path_length'] = dataset['url'].apply(lambda x: len(re.findall('/', x)))\n",
    "dataset['path_depth'] = dataset['url'].apply(lambda x: x.count('/'))\n",
    "\n",
    "# Extracting character-based features\n",
    "special_characters = '@/%/$/=/./_/?/:/~/'\n",
    "dataset['special_chars_count'] = dataset['url'].apply(lambda x: sum(x.count(char) for char in special_characters))\n",
    "\n",
    "# Combine features\n",
    "X = dataset[['domain_length', 'subdomains', 'has_ip', 'path_length', 'path_depth', 'special_chars_count']].values\n",
    "\n",
    "# Splitting the dataset into training and testing sets\n",
    "X_train, X_test = train_test_split(X, test_size=0.2, random_state=42)\n",
    "\n",
    "# Initialize the Isolation Forest model\n",
    "isolation_forest = IsolationForest(contamination=0.1, random_state=42)\n",
    "\n",
    "# Fit the model\n",
    "isolation_forest.fit(X_train)\n",
    "\n",
    "# Predictions on the testing set\n",
    "y_pred = isolation_forest.predict(X_test)\n",
    "\n",
    "# Evaluation\n",
    "print(\"Number of anomalies detected:\", len(y_pred[y_pred == -1]))\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. The (extract_features function) preprocesses the new URL and extracts the same features used in training the model.\n",
    "2. The features extracted from the new URL are transformed into a numpy array with the same shape as the training data.\n",
    "3. The Isolation Forest model predicts whether the new URL is an anomaly (-1) or not (1).\n",
    "4. Based on the prediction, it prints whether the URL is detected as an anomaly or not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The URL is not detected as an anomaly.\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import numpy as np\n",
    "\n",
    "# Function to preprocess and extract features from a URL\n",
    "def extract_features(url):\n",
    "    domain_length = len(re.findall('/.', url))\n",
    "    subdomains = url.count('.') - 1\n",
    "    has_ip = 1 if re.match(r'\\b\\d{1,3}\\.\\d{1,3}\\.\\d{1,3}\\.\\d{1,3}\\b', url) else 0\n",
    "    path_length = len(re.findall('/', url))\n",
    "    path_depth = url.count('/')\n",
    "    special_characters = '@/%/$/=/./_/?/:/~/'\n",
    "    special_chars_count = sum(url.count(char) for char in special_characters)\n",
    "    \n",
    "    return [domain_length, subdomains, has_ip, path_length, path_depth, special_chars_count]\n",
    "\n",
    "# URL to test\n",
    "new_url = \"https://certificate.doenets.lk/\"\n",
    "\n",
    "# Extract features from the new URL\n",
    "new_url_features = extract_features(new_url)\n",
    "\n",
    "# Transform features into the same format as the training data\n",
    "new_url_features_array = np.array(new_url_features).reshape(1, -1)\n",
    "#new commnet\n",
    "# Predict anomaly\n",
    "is_anomaly = isolation_forest.predict(new_url_features_array)\n",
    "\n",
    "if is_anomaly == -1:\n",
    "    print(\"The URL is detected as an anomaly.\")\n",
    "else:\n",
    "    print(\"The URL is not detected as an anomaly.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using Plotly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'anomaly_detection_plot.html'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "import pandas as pd\n",
    "from sklearn.ensemble import IsolationForest\n",
    "import plotly.graph_objs as go\n",
    "import plotly.offline as py\n",
    "\n",
    "# Loading the dataset\n",
    "dataset = pd.read_csv('D:/Education/PYTHON/ML-v2/Datasets/malicious_phish.csv')\n",
    "\n",
    "#FEATURE EXTRACTION\n",
    "# Extracting domain-based features\n",
    "dataset['domain_length'] = dataset['url'].apply(lambda x: len(re.findall('/.', x)))\n",
    "dataset['subdomains'] = dataset['url'].apply(lambda x: x.count('.') - 1)\n",
    "dataset['has_ip'] = dataset['url'].apply(lambda x: 1 if re.match(r'\\b\\d{1,3}\\.\\d{1,3}\\.\\d{1,3}\\.\\d{1,3}\\b', x) else 0)\n",
    "\n",
    "# Extracting path-based features\n",
    "dataset['path_length'] = dataset['url'].apply(lambda x: len(re.findall('/', x)))\n",
    "dataset['path_depth'] = dataset['url'].apply(lambda x: x.count('/'))\n",
    "\n",
    "# Extracting character-based features\n",
    "special_characters = '@/%/$/=/./_/?/:/~/'\n",
    "dataset['special_chars_count'] = dataset['url'].apply(lambda x: sum(x.count(char) for char in special_characters))\n",
    "\n",
    "# Combine features (Combines all the extracted features into a single NumPy array X)\n",
    "X = dataset[['domain_length', 'subdomains', 'has_ip', 'path_length', 'path_depth', 'special_chars_count']].values\n",
    "\n",
    "# Initialize the Isolation Forest model\n",
    "isolation_forest = IsolationForest(contamination=0.1, random_state=42)\n",
    "\n",
    "# Fit the model\n",
    "isolation_forest.fit(X)\n",
    "\n",
    "# Predictions on the dataset\n",
    "y_pred = isolation_forest.predict(X)\n",
    "\n",
    "# Convert predictions to a boolean array indicating anomalies (True) and inliers (False)\n",
    "anomalies = y_pred == -1\n",
    "\n",
    "# Filter X_train based on anomalies\n",
    "X_train_anomalies = X[anomalies]\n",
    "\n",
    "# Create a scatter plot of the dataset\n",
    "scatter = go.Scatter(x=X[:, 0], y=X[:, 1], mode='markers', \n",
    "                     marker=dict(color='blue', opacity=0.5), \n",
    "                     name='Inliers')\n",
    "\n",
    "# Create a scatter plot of anomalies\n",
    "anomaly_scatter = go.Scatter(x=X_train_anomalies[:, 0], y=X_train_anomalies[:, 1], mode='markers', \n",
    "                             marker=dict(color='red', opacity=0.5), \n",
    "                             name='Anomalies')\n",
    "\n",
    "# Layout settings\n",
    "layout = go.Layout(title='Anomaly Detection with Isolation Forest', \n",
    "                   xaxis=dict(title='Feature 1'), yaxis=dict(title='Feature 2'))\n",
    "\n",
    "# Create figure\n",
    "fig = go.Figure(data=[scatter, anomaly_scatter], layout=layout)\n",
    "\n",
    "# Save the plot as HTML file\n",
    "py.plot(fig, filename='anomaly_detection_plot.html')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ADDING MORE PARAMETERS TO THE EXISTIN CODE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CODE #1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of anomalies detected: 12712\n",
      "The URL is not detected as an anomaly.\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.ensemble import IsolationForest\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Loading the dataset\n",
    "dataset = pd.read_csv('D:/Education/PYTHON/ML-v2/Datasets/malicious_phish.csv')\n",
    "\n",
    "# Extracting domain-based features\n",
    "dataset['domain_length'] = dataset['url'].apply(lambda x: len(re.findall(r'\\.', x)))\n",
    "dataset['subdomains'] = dataset['url'].apply(lambda x: x.count('.') - 1)\n",
    "dataset['has_ip'] = dataset['url'].apply(lambda x: 1 if re.match(r'\\b\\d{1,3}\\.\\d{1,3}\\.\\d{1,3}\\.\\d{1,3}\\b', x) else 0)\n",
    "\n",
    "# Extracting path-based features\n",
    "dataset['path_length'] = dataset['url'].apply(lambda x: len(re.findall('/', x)))\n",
    "dataset['path_depth'] = dataset['url'].apply(lambda x: x.count('/'))\n",
    "\n",
    "# Extracting character-based features\n",
    "special_characters = '@/%/$/=/./_/?/:/~'\n",
    "dataset['special_chars_count'] = dataset['url'].apply(lambda x: sum(x.count(char) for char in special_characters))\n",
    "\n",
    "# Additional features\n",
    "# Extracting domain reputation (0 for unknown, 1 for reputable, -1 for suspicious/malicious)\n",
    "# Assume a simple rule for demonstration: domains containing 'phish' or 'malware' are marked as suspicious\n",
    "dataset['domain_reputation'] = dataset['url'].apply(lambda x: -1 if re.search(r'phish|malware', x, flags=re.IGNORECASE) else 1 if '.' in x else 0)\n",
    "\n",
    "# Length of top-level domain (TLD)\n",
    "dataset['tld_length'] = dataset['url'].apply(lambda x: len(re.findall(r'\\.([a-zA-Z]+)$', x)[0]) if len(re.findall(r'\\.([a-zA-Z]+)$', x)) > 0 else 0)\n",
    "\n",
    "# Presence of HTTP Redirects\n",
    "dataset['has_redirects'] = dataset['url'].apply(lambda x: 1 if '//' in x else 0)\n",
    "\n",
    "# Combine features\n",
    "X = dataset[['domain_length', 'subdomains', 'has_ip', 'path_length', 'path_depth', 'special_chars_count', 'domain_reputation', 'tld_length', 'has_redirects']].values\n",
    "\n",
    "# Splitting the dataset into training and testing sets\n",
    "X_train, X_test = train_test_split(X, test_size=0.2, random_state=42)\n",
    "\n",
    "# Initialize the Isolation Forest model\n",
    "isolation_forest = IsolationForest(contamination=0.1, random_state=42)\n",
    "\n",
    "# Fit the model\n",
    "isolation_forest.fit(X_train)\n",
    "\n",
    "# Predictions on the testing set\n",
    "y_pred = isolation_forest.predict(X_test)\n",
    "\n",
    "# Evaluation\n",
    "print(\"Number of anomalies detected:\", len(y_pred[y_pred == -1]))\n",
    "\n",
    "# Function to preprocess and extract features from a URL\n",
    "def extract_features(url):\n",
    "    domain_length = len(re.findall(r'\\.', url))\n",
    "    subdomains = url.count('.') - 1\n",
    "    has_ip = 1 if re.match(r'\\b\\d{1,3}\\.\\d{1,3}\\.\\d{1,3}\\.\\d{1,3}\\b', url) else 0\n",
    "    path_length = len(re.findall('/', url))\n",
    "    path_depth = url.count('/')\n",
    "    special_characters = '@/%/$/=/./_/?/:/~'\n",
    "    special_chars_count = sum(url.count(char) for char in special_characters)\n",
    "    domain_reputation = -1 if re.search(r'phish|malware', url, flags=re.IGNORECASE) else 1 if '.' in url else 0\n",
    "    tld_length = len(re.findall(r'\\.([a-zA-Z]+)$', url)[0]) if len(re.findall(r'\\.([a-zA-Z]+)$', url)) > 0 else 0\n",
    "    has_redirects = 1 if '//' in url else 0\n",
    "    \n",
    "    return [domain_length, subdomains, has_ip, path_length, path_depth, special_chars_count, domain_reputation, tld_length, has_redirects]\n",
    "\n",
    "# URL to test\n",
    "new_url = \"https://www.sampathvishwa.com/SVRClientWeb/ActionController\"\n",
    "\n",
    "# Extract features from the new URL\n",
    "new_url_features = extract_features(new_url)\n",
    "\n",
    "# Transform features into the same format as the training data\n",
    "new_url_features_array = np.array(new_url_features).reshape(1, -1)\n",
    "\n",
    "# Predict anomaly\n",
    "is_anomaly = isolation_forest.predict(new_url_features_array)\n",
    "\n",
    "if is_anomaly == -1:\n",
    "    print(\"The URL is detected as an anomaly.\")\n",
    "else:\n",
    "    print(\"The URL is not detected as an anomaly.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CODE #2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'status'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[1;32mc:\\Users\\ASUS\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pandas\\core\\indexes\\base.py:3805\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   3804\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 3805\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcasted_key\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   3806\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "File \u001b[1;32mindex.pyx:167\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mindex.pyx:196\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mpandas\\\\_libs\\\\hashtable_class_helper.pxi:7081\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mpandas\\\\_libs\\\\hashtable_class_helper.pxi:7089\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 'status'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 53\u001b[0m\n\u001b[0;32m     49\u001b[0m dataset[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mspecial_chars_count\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m dataset[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124murl\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mapply(\u001b[38;5;28;01mlambda\u001b[39;00m x: \u001b[38;5;28msum\u001b[39m(x\u001b[38;5;241m.\u001b[39mcount(char) \u001b[38;5;28;01mfor\u001b[39;00m char \u001b[38;5;129;01min\u001b[39;00m special_characters))\n\u001b[0;32m     51\u001b[0m \u001b[38;5;66;03m# Additional features\u001b[39;00m\n\u001b[0;32m     52\u001b[0m \u001b[38;5;66;03m# Domain Reputation\u001b[39;00m\n\u001b[1;32m---> 53\u001b[0m dataset[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdomain_reputation\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[43mdataset\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mrow\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mget_domain_reputation\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrow\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43murl\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msplit\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m//\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msplit\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m/\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mrow\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mstatus\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m==\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mphishing\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     55\u001b[0m \u001b[38;5;66;03m# Presence of Subdomain Keywords\u001b[39;00m\n\u001b[0;32m     56\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m keyword \u001b[38;5;129;01min\u001b[39;00m suspicious_subdomain_keywords:\n",
      "File \u001b[1;32mc:\\Users\\ASUS\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pandas\\core\\frame.py:10361\u001b[0m, in \u001b[0;36mDataFrame.apply\u001b[1;34m(self, func, axis, raw, result_type, args, by_row, engine, engine_kwargs, **kwargs)\u001b[0m\n\u001b[0;32m  10347\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mapply\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m frame_apply\n\u001b[0;32m  10349\u001b[0m op \u001b[38;5;241m=\u001b[39m frame_apply(\n\u001b[0;32m  10350\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m  10351\u001b[0m     func\u001b[38;5;241m=\u001b[39mfunc,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m  10359\u001b[0m     kwargs\u001b[38;5;241m=\u001b[39mkwargs,\n\u001b[0;32m  10360\u001b[0m )\n\u001b[1;32m> 10361\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mop\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39m__finalize__(\u001b[38;5;28mself\u001b[39m, method\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mapply\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\ASUS\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pandas\\core\\apply.py:916\u001b[0m, in \u001b[0;36mFrameApply.apply\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    913\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mraw:\n\u001b[0;32m    914\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mapply_raw(engine\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mengine, engine_kwargs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mengine_kwargs)\n\u001b[1;32m--> 916\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply_standard\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\ASUS\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pandas\\core\\apply.py:1063\u001b[0m, in \u001b[0;36mFrameApply.apply_standard\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1061\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mapply_standard\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m   1062\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mengine \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpython\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m-> 1063\u001b[0m         results, res_index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply_series_generator\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1064\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1065\u001b[0m         results, res_index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mapply_series_numba()\n",
      "File \u001b[1;32mc:\\Users\\ASUS\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pandas\\core\\apply.py:1081\u001b[0m, in \u001b[0;36mFrameApply.apply_series_generator\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1078\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m option_context(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmode.chained_assignment\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m   1079\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i, v \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(series_gen):\n\u001b[0;32m   1080\u001b[0m         \u001b[38;5;66;03m# ignore SettingWithCopy here in case the user mutates\u001b[39;00m\n\u001b[1;32m-> 1081\u001b[0m         results[i] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1082\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(results[i], ABCSeries):\n\u001b[0;32m   1083\u001b[0m             \u001b[38;5;66;03m# If we have a view on v, we need to make a copy because\u001b[39;00m\n\u001b[0;32m   1084\u001b[0m             \u001b[38;5;66;03m#  series_generator will swap out the underlying data\u001b[39;00m\n\u001b[0;32m   1085\u001b[0m             results[i] \u001b[38;5;241m=\u001b[39m results[i]\u001b[38;5;241m.\u001b[39mcopy(deep\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "Cell \u001b[1;32mIn[4], line 53\u001b[0m, in \u001b[0;36m<lambda>\u001b[1;34m(row)\u001b[0m\n\u001b[0;32m     49\u001b[0m dataset[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mspecial_chars_count\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m dataset[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124murl\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mapply(\u001b[38;5;28;01mlambda\u001b[39;00m x: \u001b[38;5;28msum\u001b[39m(x\u001b[38;5;241m.\u001b[39mcount(char) \u001b[38;5;28;01mfor\u001b[39;00m char \u001b[38;5;129;01min\u001b[39;00m special_characters))\n\u001b[0;32m     51\u001b[0m \u001b[38;5;66;03m# Additional features\u001b[39;00m\n\u001b[0;32m     52\u001b[0m \u001b[38;5;66;03m# Domain Reputation\u001b[39;00m\n\u001b[1;32m---> 53\u001b[0m dataset[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdomain_reputation\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m dataset\u001b[38;5;241m.\u001b[39mapply(\u001b[38;5;28;01mlambda\u001b[39;00m row: get_domain_reputation(row[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124murl\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m//\u001b[39m\u001b[38;5;124m'\u001b[39m)[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/\u001b[39m\u001b[38;5;124m'\u001b[39m)[\u001b[38;5;241m0\u001b[39m]) \u001b[38;5;28;01mif\u001b[39;00m \u001b[43mrow\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mstatus\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mphishing\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;241m0\u001b[39m, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m     55\u001b[0m \u001b[38;5;66;03m# Presence of Subdomain Keywords\u001b[39;00m\n\u001b[0;32m     56\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m keyword \u001b[38;5;129;01min\u001b[39;00m suspicious_subdomain_keywords:\n",
      "File \u001b[1;32mc:\\Users\\ASUS\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pandas\\core\\series.py:1112\u001b[0m, in \u001b[0;36mSeries.__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   1109\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_values[key]\n\u001b[0;32m   1111\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m key_is_scalar:\n\u001b[1;32m-> 1112\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_value\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1114\u001b[0m \u001b[38;5;66;03m# Convert generator to list before going through hashable part\u001b[39;00m\n\u001b[0;32m   1115\u001b[0m \u001b[38;5;66;03m# (We will iterate through the generator there to check for slices)\u001b[39;00m\n\u001b[0;32m   1116\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_iterator(key):\n",
      "File \u001b[1;32mc:\\Users\\ASUS\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pandas\\core\\series.py:1228\u001b[0m, in \u001b[0;36mSeries._get_value\u001b[1;34m(self, label, takeable)\u001b[0m\n\u001b[0;32m   1225\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_values[label]\n\u001b[0;32m   1227\u001b[0m \u001b[38;5;66;03m# Similar to Index.get_value, but we do not fall back to positional\u001b[39;00m\n\u001b[1;32m-> 1228\u001b[0m loc \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mindex\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlabel\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1230\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_integer(loc):\n\u001b[0;32m   1231\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_values[loc]\n",
      "File \u001b[1;32mc:\\Users\\ASUS\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pandas\\core\\indexes\\base.py:3812\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   3807\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(casted_key, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m (\n\u001b[0;32m   3808\u001b[0m         \u001b[38;5;28misinstance\u001b[39m(casted_key, abc\u001b[38;5;241m.\u001b[39mIterable)\n\u001b[0;32m   3809\u001b[0m         \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28many\u001b[39m(\u001b[38;5;28misinstance\u001b[39m(x, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m casted_key)\n\u001b[0;32m   3810\u001b[0m     ):\n\u001b[0;32m   3811\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m InvalidIndexError(key)\n\u001b[1;32m-> 3812\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01merr\u001b[39;00m\n\u001b[0;32m   3813\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[0;32m   3814\u001b[0m     \u001b[38;5;66;03m# If we have a listlike key, _check_indexing_error will raise\u001b[39;00m\n\u001b[0;32m   3815\u001b[0m     \u001b[38;5;66;03m#  InvalidIndexError. Otherwise we fall through and re-raise\u001b[39;00m\n\u001b[0;32m   3816\u001b[0m     \u001b[38;5;66;03m#  the TypeError.\u001b[39;00m\n\u001b[0;32m   3817\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_indexing_error(key)\n",
      "\u001b[1;31mKeyError\u001b[0m: 'status'"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import requests\n",
    "from sklearn.ensemble import IsolationForest\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "# Mock function for domain reputation (simply returns 1 for reputable, -1 for suspicious/malicious)\n",
    "def get_domain_reputation(domain):\n",
    "    suspicious_domains = ['phish', 'malware', 'scam', 'fraud']\n",
    "    if any(keyword in domain for keyword in suspicious_domains):\n",
    "        return -1\n",
    "    else:\n",
    "        return 1\n",
    "\n",
    "# Predefined list of suspicious subdomain keywords\n",
    "suspicious_subdomain_keywords = ['login', 'verify', 'account', 'secure']\n",
    "\n",
    "# Function to check SSL certificate\n",
    "def check_ssl_certificate(url):\n",
    "    try:\n",
    "        response = requests.head(url, verify=True)\n",
    "        if response.status_code == 200 and response.headers.get('Strict-Transport-Security'):\n",
    "            return 1  # SSL certificate is present\n",
    "        else:\n",
    "            return 0  # SSL certificate is not present\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"Error checking SSL certificate for URL: {url}\")\n",
    "        print(f\"Error message: {str(e)}\")\n",
    "        return 0  # Assume SSL certificate is not present due to error\n",
    "\n",
    "\n",
    "# Loading the dataset\n",
    "dataset = pd.read_csv('D:/Education/PYTHON/ML-v2/Datasets/malicious_phish.csv')\n",
    "\n",
    "\n",
    "# Extracting domain-based features\n",
    "dataset['domain_length'] = dataset['url'].apply(lambda x: len(re.findall(r'\\.', x)))\n",
    "dataset['subdomains'] = dataset['url'].apply(lambda x: x.count('.') - 1)\n",
    "dataset['has_ip'] = dataset['url'].apply(lambda x: 1 if re.match(r'\\b\\d{1,3}\\.\\d{1,3}\\.\\d{1,3}\\.\\d{1,3}\\b', x) else 0)\n",
    "\n",
    "# Extracting path-based features\n",
    "dataset['path_length'] = dataset['url'].apply(lambda x: len(re.findall('/', x)))\n",
    "dataset['path_depth'] = dataset['url'].apply(lambda x: x.count('/'))\n",
    "\n",
    "# Extracting character-based features\n",
    "special_characters = '@/%/$/=/./_/?/:/~'\n",
    "dataset['special_chars_count'] = dataset['url'].apply(lambda x: sum(x.count(char) for char in special_characters))\n",
    "\n",
    "# Additional features\n",
    "# Domain Reputation\n",
    "dataset['domain_reputation'] = dataset.apply(lambda row: get_domain_reputation(row['url'].split('//')[-1].split('/')[0]) if row['status'] == 'phishing' else 0, axis=1)\n",
    "\n",
    "# Presence of Subdomain Keywords\n",
    "for keyword in suspicious_subdomain_keywords:\n",
    "    dataset['has_' + keyword + '_subdomain'] = dataset.apply(lambda row: 1 if keyword in row['url'].split('//')[-1].split('/')[0] and row['status'] == 'phishing' else 0, axis=1)\n",
    "\n",
    "# SSL Certificate\n",
    "dataset['has_ssl_certificate'] = dataset.apply(lambda row: 1 if check_ssl_certificate(row['url']) and row['status'] == 'phishing' else 0, axis=1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "\"['has_ssl_certificate'] not in index\"",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Combine features\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m X \u001b[38;5;241m=\u001b[39m \u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mdomain_length\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43msubdomains\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mhas_ip\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mpath_length\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mpath_depth\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mspecial_chars_count\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m      3\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mdomain_reputation\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mhas_ssl_certificate\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mhas_\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mkeyword\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m_subdomain\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mkeyword\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43msuspicious_subdomain_keywords\u001b[49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;241m.\u001b[39mvalues\n\u001b[0;32m      5\u001b[0m \u001b[38;5;66;03m# Splitting the dataset into training and testing sets\u001b[39;00m\n\u001b[0;32m      6\u001b[0m X_train, X_test \u001b[38;5;241m=\u001b[39m train_test_split(X, test_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.2\u001b[39m, random_state\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m42\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\ASUS\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pandas\\core\\frame.py:4096\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   4094\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m is_iterator(key):\n\u001b[0;32m   4095\u001b[0m         key \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(key)\n\u001b[1;32m-> 4096\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_indexer_strict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcolumns\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m[\u001b[38;5;241m1\u001b[39m]\n\u001b[0;32m   4098\u001b[0m \u001b[38;5;66;03m# take() does not accept boolean indexers\u001b[39;00m\n\u001b[0;32m   4099\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(indexer, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdtype\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mbool\u001b[39m:\n",
      "File \u001b[1;32mc:\\Users\\ASUS\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pandas\\core\\indexes\\base.py:6200\u001b[0m, in \u001b[0;36mIndex._get_indexer_strict\u001b[1;34m(self, key, axis_name)\u001b[0m\n\u001b[0;32m   6197\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   6198\u001b[0m     keyarr, indexer, new_indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reindex_non_unique(keyarr)\n\u001b[1;32m-> 6200\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_raise_if_missing\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkeyarr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindexer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis_name\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   6202\u001b[0m keyarr \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtake(indexer)\n\u001b[0;32m   6203\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(key, Index):\n\u001b[0;32m   6204\u001b[0m     \u001b[38;5;66;03m# GH 42790 - Preserve name from an Index\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\ASUS\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pandas\\core\\indexes\\base.py:6252\u001b[0m, in \u001b[0;36mIndex._raise_if_missing\u001b[1;34m(self, key, indexer, axis_name)\u001b[0m\n\u001b[0;32m   6249\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNone of [\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mkey\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m] are in the [\u001b[39m\u001b[38;5;132;01m{\u001b[39;00maxis_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m]\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m   6251\u001b[0m not_found \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(ensure_index(key)[missing_mask\u001b[38;5;241m.\u001b[39mnonzero()[\u001b[38;5;241m0\u001b[39m]]\u001b[38;5;241m.\u001b[39munique())\n\u001b[1;32m-> 6252\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnot_found\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m not in index\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mKeyError\u001b[0m: \"['has_ssl_certificate'] not in index\""
     ]
    }
   ],
   "source": [
    "# Combine features\n",
    "X = dataset[['domain_length', 'subdomains', 'has_ip', 'path_length', 'path_depth', 'special_chars_count', \n",
    "            'domain_reputation', 'has_ssl_certificate'] + ['has_' + keyword + '_subdomain' for keyword in suspicious_subdomain_keywords]].values\n",
    "\n",
    "# Splitting the dataset into training and testing sets\n",
    "X_train, X_test = train_test_split(X, test_size=0.2, random_state=42)\n",
    "\n",
    "# Initialize the Isolation Forest model\n",
    "isolation_forest = IsolationForest(contamination=0.1, random_state=42)\n",
    "\n",
    "# Fit the model\n",
    "isolation_forest.fit(X_train)\n",
    "\n",
    "# Predictions on the testing set\n",
    "y_pred = isolation_forest.predict(X_test)\n",
    "\n",
    "# Evaluation\n",
    "print(\"Number of anomalies detected:\", len(y_pred[y_pred == -1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'isolation_forest' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 27\u001b[0m\n\u001b[0;32m     24\u001b[0m new_url_features_array \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray(new_url_features)\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m     26\u001b[0m \u001b[38;5;66;03m# Predict anomaly\u001b[39;00m\n\u001b[1;32m---> 27\u001b[0m is_anomaly \u001b[38;5;241m=\u001b[39m \u001b[43misolation_forest\u001b[49m\u001b[38;5;241m.\u001b[39mpredict(new_url_features_array)\n\u001b[0;32m     29\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_anomaly \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m     30\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe URL is detected as an anomaly.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'isolation_forest' is not defined"
     ]
    }
   ],
   "source": [
    "# Function to preprocess and extract features from a URL\n",
    "def extract_features(url):\n",
    "    domain_length = len(re.findall(r'\\.', url))\n",
    "    subdomains = url.count('.') - 1\n",
    "    has_ip = 1 if re.match(r'\\b\\d{1,3}\\.\\d{1,3}\\.\\d{1,3}\\.\\d{1,3}\\b', url) else 0\n",
    "    path_length = len(re.findall('/', url))\n",
    "    path_depth = url.count('/')\n",
    "    special_characters = '@/%/$/=/./_/?/:/~'\n",
    "    special_chars_count = sum(url.count(char) for char in special_characters)\n",
    "    domain_reputation = get_domain_reputation(url.split('//')[-1].split('/')[0])\n",
    "    has_ssl_certificate = 1 if check_ssl_certificate(url) else 0\n",
    "    subdomain_keyword_features = [1 if keyword in url.split('//')[-1].split('/')[0] else 0 for keyword in suspicious_subdomain_keywords]\n",
    "    \n",
    "    return [domain_length, subdomains, has_ip, path_length, path_depth, special_chars_count, \n",
    "            domain_reputation, has_ssl_certificate] + subdomain_keyword_features\n",
    "\n",
    "# URL to test\n",
    "new_url = \"en.wikipedia.org/wiki/\"\n",
    "\n",
    "# Extract features from the new URL\n",
    "new_url_features = extract_features(new_url)\n",
    "\n",
    "# Transform features into the same format as the training data\n",
    "new_url_features_array = np.array(new_url_features).reshape(1, -1)\n",
    "\n",
    "# Predict anomaly\n",
    "is_anomaly = isolation_forest.predict(new_url_features_array)\n",
    "\n",
    "if is_anomaly == -1:\n",
    "    print(\"The URL is detected as an anomaly.\")\n",
    "else:\n",
    "    print(\"The URL is not detected as an anomaly.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
