{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "My Second Notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Code #2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "import re\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Load the dataset\n",
    "dataset = pd.read_csv('D:/Education/PYTHON/ML-v2/Datasets/malicious_phish.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocessing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extracting domain-based features\n",
    "dataset['domain_length'] = dataset['url'].apply(lambda x: len(re.findall('/.', x)))\n",
    "dataset['subdomains'] = dataset['url'].apply(lambda x: x.count('.') - 1)\n",
    "dataset['tld'] = dataset['url'].apply(lambda x: x.split('.')[-1])\n",
    "dataset['has_ip'] = dataset['url'].apply(lambda x: 1 if re.match(r'\\b\\d{1,3}\\.\\d{1,3}\\.\\d{1,3}\\.\\d{1,3}\\b', x) else 0)\n",
    "\n",
    "# Extracting path-based features\n",
    "dataset['path_length'] = dataset['url'].apply(lambda x: len(re.findall('/', x)))\n",
    "dataset['path_depth'] = dataset['url'].apply(lambda x: x.count('/'))\n",
    "\n",
    "# Extracting character-based features\n",
    "special_characters = '@/%/$/=/./_/?/:/~/'\n",
    "dataset['special_chars_count'] = dataset['url'].apply(lambda x: sum(x.count(char) for char in special_characters))\n",
    "\n",
    "# Encoding categorical labels to numerical format\n",
    "label_mapping = {'benign': 0, 'defacement': 1, 'phishing': 2, 'malware': 3}\n",
    "dataset['type'] = dataset['type'].map(label_mapping)\n",
    "\n",
    "# Feature Extraction using TF-IDF vectorization\n",
    "tfidf_vectorizer = TfidfVectorizer(max_features=1000)\n",
    "X_text_features = tfidf_vectorizer.fit_transform(dataset['url'])\n",
    "\n",
    "# Combining text features with extracted features\n",
    "X_numeric_features = dataset[['domain_length', 'subdomains', 'has_ip', 'path_length', 'path_depth', 'special_chars_count']].values\n",
    "X = pd.concat([pd.DataFrame(X_text_features.toarray()), pd.DataFrame(X_numeric_features)], axis=1)\n",
    "y = dataset['type']\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9766045500963613\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.99      0.99     85778\n",
      "           1       0.98      0.99      0.99     19104\n",
      "           2       0.95      0.90      0.92     18836\n",
      "           3       0.99      0.95      0.97      6521\n",
      "\n",
      "    accuracy                           0.98    130239\n",
      "   macro avg       0.97      0.96      0.97    130239\n",
      "weighted avg       0.98      0.98      0.98    130239\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Splitting the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Initialize the Random Forest classifier\n",
    "rf_classifier = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "\n",
    "# Train the classifier\n",
    "rf_classifier.fit(X_train, y_train)\n",
    "\n",
    "# Predictions on the testing set\n",
    "y_pred = rf_classifier.predict(X_test)\n",
    "\n",
    "# Evaluation\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Code #3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9756678107172199\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.99      0.99     85778\n",
      "           1       0.98      0.99      0.99     19104\n",
      "           2       0.94      0.90      0.92     18836\n",
      "           3       0.99      0.95      0.97      6521\n",
      "\n",
      "    accuracy                           0.98    130239\n",
      "   macro avg       0.97      0.96      0.97    130239\n",
      "weighted avg       0.98      0.98      0.98    130239\n",
      "\n",
      "\n",
      "Predictions for new URLs:\n",
      "URL: http://www.google.com - Predicted Label: phishing\n",
      "URL: https://example.com - Predicted Label: phishing\n",
      "URL: http://www.phishing-site.com - Predicted Label: phishing\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "\n",
    "# Loading the dataset\n",
    "dataset = pd.read_csv('D:/Education/PYTHON/ML-v2/Datasets/malicious_phish.csv')\n",
    "\n",
    "# Extracting domain-based features\n",
    "dataset['domain_length'] = dataset['url'].apply(lambda x: len(re.findall('/.', x)))\n",
    "dataset['subdomains'] = dataset['url'].apply(lambda x: x.count('.') - 1)\n",
    "dataset['tld'] = dataset['url'].apply(lambda x: x.split('.')[-1])\n",
    "dataset['has_ip'] = dataset['url'].apply(lambda x: 1 if re.match(r'\\b\\d{1,3}\\.\\d{1,3}\\.\\d{1,3}\\.\\d{1,3}\\b', x) else 0)\n",
    "\n",
    "# Extracting path-based features\n",
    "dataset['path_length'] = dataset['url'].apply(lambda x: len(re.findall('/', x)))\n",
    "dataset['path_depth'] = dataset['url'].apply(lambda x: x.count('/'))\n",
    "\n",
    "# Extracting character-based features\n",
    "special_characters = '@/%/$'\n",
    "dataset['special_chars_count'] = dataset['url'].apply(lambda x: sum(x.count(char) for char in special_characters))\n",
    "\n",
    "# Encoding categorical labels to numerical format\n",
    "label_mapping = {'benign': 0, 'defacement': 1, 'phishing': 2, 'malware': 3}\n",
    "dataset['type'] = dataset['type'].map(label_mapping)\n",
    "\n",
    "# Feature Extraction using TF-IDF vectorization\n",
    "tfidf_vectorizer = TfidfVectorizer(max_features=1000)\n",
    "X_text_features = tfidf_vectorizer.fit_transform(dataset['url'])\n",
    "\n",
    "# Combining text features with extracted features\n",
    "X_numeric_features = dataset[['domain_length', 'subdomains', 'has_ip', 'path_length', 'path_depth', 'special_chars_count']].values\n",
    "X = pd.concat([pd.DataFrame(X_text_features.toarray()), pd.DataFrame(X_numeric_features)], axis=1)\n",
    "y = dataset['type']\n",
    "\n",
    "# Splitting the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Initialize the Random Forest classifier\n",
    "rf_classifier = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "\n",
    "# Train the classifier\n",
    "rf_classifier.fit(X_train, y_train)\n",
    "\n",
    "# Predictions on the testing set\n",
    "y_pred = rf_classifier.predict(X_test)\n",
    "\n",
    "# Evaluation\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "# Example new URLs\n",
    "new_urls = pd.DataFrame({'url': ['http://www.google.com', 'https://example.com', 'http://www.phishing-site.com']})\n",
    "\n",
    "# Define a function to extract features from new URLs\n",
    "def extract_features(new_urls):\n",
    "    new_urls['domain_length'] = new_urls['url'].apply(lambda x: len(re.findall('/.', x)))\n",
    "    new_urls['subdomains'] = new_urls['url'].apply(lambda x: x.count('.') - 1)\n",
    "    new_urls['has_ip'] = new_urls['url'].apply(lambda x: 1 if re.match(r'\\b\\d{1,3}\\.\\d{1,3}\\.\\d{1,3}\\.\\d{1,3}\\b', x) else 0)\n",
    "    new_urls['path_length'] = new_urls['url'].apply(lambda x: len(re.findall('/', x)))\n",
    "    new_urls['path_depth'] = new_urls['url'].apply(lambda x: x.count('/'))\n",
    "    special_characters = '@/%/$'\n",
    "    new_urls['special_chars_count'] = new_urls['url'].apply(lambda x: sum(x.count(char) for char in special_characters))\n",
    "    return new_urls\n",
    "\n",
    "# Extract features from new URLs\n",
    "new_urls = extract_features(new_urls)\n",
    "\n",
    "# TF-IDF vectorization\n",
    "X_text_features_new = tfidf_vectorizer.transform(new_urls['url'])\n",
    "\n",
    "# Combine text features with extracted features\n",
    "X_numeric_features_new = new_urls[['domain_length', 'subdomains', 'has_ip', 'path_length', 'path_depth', 'special_chars_count']].values\n",
    "X_new = pd.concat([pd.DataFrame(X_text_features_new.toarray()), pd.DataFrame(X_numeric_features_new)], axis=1)\n",
    "\n",
    "# Predictions on the new URLs\n",
    "y_pred_new = rf_classifier.predict(X_new)\n",
    "\n",
    "# Decode numerical labels to original categories\n",
    "reverse_label_mapping = {v: k for k, v in label_mapping.items()}\n",
    "predicted_labels = [reverse_label_mapping[label] for label in y_pred_new]\n",
    "\n",
    "print(\"\\nPredictions for new URLs:\")\n",
    "for url, pred_label in zip(new_urls['url'], predicted_labels):\n",
    "    print(f\"URL: {url} - Predicted Label: {pred_label}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Code #4 (SVMs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9766045500963613\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.99      0.99     85778\n",
      "           1       0.98      0.99      0.99     19104\n",
      "           2       0.95      0.90      0.92     18836\n",
      "           3       0.99      0.95      0.97      6521\n",
      "\n",
      "    accuracy                           0.98    130239\n",
      "   macro avg       0.97      0.96      0.97    130239\n",
      "weighted avg       0.98      0.98      0.98    130239\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "\n",
    "# Loading the dataset\n",
    "dataset = pd.read_csv('D:/Education/PYTHON/ML-v2/Datasets/malicious_phish.csv')\n",
    "\n",
    "# Extracting domain-based features\n",
    "dataset['domain_length'] = dataset['url'].apply(lambda x: len(re.findall('/.', x)))\n",
    "dataset['subdomains'] = dataset['url'].apply(lambda x: x.count('.') - 1)\n",
    "dataset['tld'] = dataset['url'].apply(lambda x: x.split('.')[-1])\n",
    "dataset['has_ip'] = dataset['url'].apply(lambda x: 1 if re.match(r'\\b\\d{1,3}\\.\\d{1,3}\\.\\d{1,3}\\.\\d{1,3}\\b', x) else 0)\n",
    "\n",
    "# Extracting path-based features\n",
    "dataset['path_length'] = dataset['url'].apply(lambda x: len(re.findall('/', x)))\n",
    "dataset['path_depth'] = dataset['url'].apply(lambda x: x.count('/'))\n",
    "\n",
    "# Extracting character-based features\n",
    "special_characters = '@/%/$/=/./_/?/:/~/'\n",
    "dataset['special_chars_count'] = dataset['url'].apply(lambda x: sum(x.count(char) for char in special_characters))\n",
    "\n",
    "# Encoding categorical labels to numerical format\n",
    "label_mapping = {'benign': 0, 'defacement': 1, 'phishing': 2, 'malware': 3}\n",
    "dataset['type'] = dataset['type'].map(label_mapping)\n",
    "\n",
    "# Feature Extraction using TF-IDF vectorization\n",
    "tfidf_vectorizer = TfidfVectorizer(max_features=1000)\n",
    "X_text_features = tfidf_vectorizer.fit_transform(dataset['url'])\n",
    "\n",
    "# Combining text features with extracted features\n",
    "X_numeric_features = dataset[['domain_length', 'subdomains', 'has_ip', 'path_length', 'path_depth', 'special_chars_count']].values\n",
    "X = pd.concat([pd.DataFrame(X_text_features.toarray()), pd.DataFrame(X_numeric_features)], axis=1)\n",
    "y = dataset['type']\n",
    "\n",
    "# Splitting the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Initialize the Random Forest classifier\n",
    "rf_classifier = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "\n",
    "# Train the classifier\n",
    "rf_classifier.fit(X_train, y_train)\n",
    "\n",
    "# Predictions on the testing set\n",
    "y_pred = rf_classifier.predict(X_test)\n",
    "\n",
    "# Evaluation\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example new URLs\n",
    "new_urls = pd.DataFrame({'url': ['https://www.kaspersky.co.in/', 'https://us.norton.com/', 'https://t.me/+yK4o5P3HqhxhYzU8']})\n",
    "\n",
    "# Define a function to extract features from new URLs\n",
    "def extract_features(new_urls):\n",
    "    new_urls['domain_length'] = new_urls['url'].apply(lambda x: len(re.findall('/.', x)))\n",
    "    new_urls['subdomains'] = new_urls['url'].apply(lambda x: x.count('.') - 1)\n",
    "    new_urls['has_ip'] = new_urls['url'].apply(lambda x: 1 if re.match(r'\\b\\d{1,3}\\.\\d{1,3}\\.\\d{1,3}\\.\\d{1,3}\\b', x) else 0)\n",
    "    new_urls['path_length'] = new_urls['url'].apply(lambda x: len(re.findall('/', x)))\n",
    "    new_urls['path_depth'] = new_urls['url'].apply(lambda x: x.count('/'))\n",
    "    special_characters = '@/%/$/=/./_/?/:/~/'\n",
    "    new_urls['special_chars_count'] = new_urls['url'].apply(lambda x: sum(x.count(char) for char in special_characters))\n",
    "    return new_urls\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Predictions for new URLs:\n",
      "URL: https://www.kaspersky.co.in/ - Predicted Label: phishing\n",
      "URL: https://us.norton.com/ - Predicted Label: phishing\n",
      "URL: https://t.me/+yK4o5P3HqhxhYzU8 - Predicted Label: phishing\n"
     ]
    }
   ],
   "source": [
    "# Extract features from new URLs\n",
    "new_urls = extract_features(new_urls)\n",
    "\n",
    "# TF-IDF vectorization\n",
    "X_text_features_new = tfidf_vectorizer.transform(new_urls['url'])\n",
    "\n",
    "# Combine text features with extracted features\n",
    "X_numeric_features_new = new_urls[['domain_length', 'subdomains', 'has_ip', 'path_length', 'path_depth', 'special_chars_count']].values\n",
    "X_new = pd.concat([pd.DataFrame(X_text_features_new.toarray()), pd.DataFrame(X_numeric_features_new)], axis=1)\n",
    "\n",
    "# Predictions on the new URLs\n",
    "y_pred_new = rf_classifier.predict(X_new)\n",
    "\n",
    "# Decode numerical labels to original categories\n",
    "reverse_label_mapping = {v: k for k, v in label_mapping.items()}\n",
    "predicted_labels = [reverse_label_mapping[label] for label in y_pred_new]\n",
    "\n",
    "print(\"\\nPredictions for new URLs:\")\n",
    "for url, pred_label in zip(new_urls['url'], predicted_labels):\n",
    "    print(f\"URL: {url} - Predicted Label: {pred_label}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
